---
title: "Econometrics - Homework Assignment"
author: Maïté Lamothe - Florentine Oliveira - Tom Verrier
date: "10/11/2019"
output:
   pdf_document:
      fig_caption: true
      number_sections: false
---
\newpage
\tableofcontents
\newpage

```{r, include = FALSE, warning=FALSE, message=FALSE, error=FALSE} 
list.of.packages <- c("tidyverse", "knitr", "haven", "stargazer", "dummies", "car", "AER", "dplyr", "readxl", "lmtest", "sandwich")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos = "http://cran.us.r-project.org")

library("tidyverse")
library("dplyr")
library("knitr")
library("haven")
library("stargazer")
library("dummies")
library("readxl")
library("lmtest")
library("sandwich")

df_wages <- read_excel("data/DataSetWages.xlsx")
df_rates <- read_excel("data/DataSetInterestRates.xlsx")
```



# 1] Description of the sample

\vspace{20pt}

## Question 1

```{r, include = FALSE}
table1 <- df_wages %>%
  select(SALARY, SALBEGIN, EDUC) %>%
  gather(key = variable, value = value, SALARY:SALBEGIN:EDUC) %>%
  group_by(variable) %>%
  summarise( 
            Mean=mean(value, na.rm=T), 
            Sd=sd(value, na.rm=T),
            Min=min(value, na.rm=T),
            Max=max(value, na.rm=T)) %>%
  ungroup(variable) 

table2 <- df_wages %>%
  select(GENDER, MINORITY) %>%
  gather(key = variable, value = value, GENDER:MINORITY) %>%
  group_by(variable) %>%
  summarise( 
            Mean=mean(value, na.rm=T)) %>%
  ungroup(variable) 
```

- interprération écart type EDUC 
- comparaison sal et salbegin : gros écart d'écart type, différence éducation, salaire d'efficience (mean)
- interprétations sur gender et minotity: sample composed of more women ou jsp + regarder interprétation  dummies gender et tout : much diversity 
+ ajouter tableau stats jobcat + commenter


```{r, echo = FALSE}
kable(table1, caption = 'Some statistics')
kable(table2, caption = 'Some other statistics')
```

\vspace{20pt}

## Question 2

```{r , fig.width = 4, fig.height = 3, warning = FALSE, echo = FALSE, error = FALSE, message = FALSE, fig.align='center'}
plot_salary <- ggplot(df_wages, aes(x=SALARY))  +
  theme_bw() +
  labs(x = "Salary") +
  labs(y="Density") +
  labs(title = "Distribution of the salary") +
  geom_density(colour="white", fill="red") 

plot_salary
```

It could be a good idea to use the logarithm of the variable Salary because the logarithm linearizes and smoothes the variable. In facts it decreases the expanse of the values that the variable takes (max-min is lower). Moreover, the interest to use the logarithm of this variable is that we can easily interpret the coefficient as an elasticity in a log-log model.

\vspace{20pt}

## Question 3

```{r , fig.width = 4, fig.height = 3, warning = FALSE, echo = FALSE, error = FALSE, message = FALSE, fig.align='center'}
plot_LogSalary <- ggplot(df_wages, aes(x=LOGSAL))  +
  theme_bw() +
  labs(x = "Log(salary)") +
  labs(y="Density") +
  labs(title = "Distribution of the Log(salary)") +
  geom_density(colour="white", fill="red") 

plot_LogSalary
```

We see that the variable LogSal is much more readable. The distribution is less extensive.

\newpage

# 2] Linear Regression

\vspace{20pt}

## Question 1

### a)

We estimate the model: $LogSal = \alpha + \beta Education + \epsilon$ (**R1**)

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
R1 <- lm(LOGSAL ~ EDUC, data = df_wages)
stargazer(R1,  header=FALSE, type='latex')
```

We can see on the *Table 2* that the variable Education is statistically significant at the threshold of 1%, as well as the intercept.

### b)

This is a log-level linear model. Then, an increase in one year of education lead to an increase of 100.$\hat\beta = 100*0.096 = 9,6$ in the Salary.

### c)

\vspace{20pt}

## Question 2

We now estimate the model: $LogSal = \alpha + \beta_1 Education + \beta_2 LogSalBegin + \epsilon$ (**R2**). The results are shown in *Table 4*.

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
R2 <- lm(LOGSAL ~ EDUC + LOGSALBEGIN, data = df_wages)
stargazer(R2,  header=FALSE, type='latex')
```

### a) 

The impact of education on LogSal is different from the first model **R1** because we have added an explanatory variable in the model.      
Mathematically, the matrix X of the explanatory variable is now different. Therefore the vector of estimated coefficients, which is equal to $(X'X)^{-1}X'Y$, differs. There was an omitted variable bias.

### b)

Theoretically:
total effect is the effect shown with model R1
direct effect is the one of model R2
the indirect effect is the one captures by the regression : logsalbegin on a constant and Education
We are supposed to find that R1=R2+ 

### c)

We regress $LogSalBegin$ on a constant and $Education$. Results are shown in *Table 5*.

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
I1 <- lm(LOGSALBEGIN ~ EDUC, data = df_wages)
stargazer(I1,  header=FALSE, type='latex')
```

### d) 

\vspace{20pt}

## Question 3

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE , include = FALSE}
I2 <- lm(LOGSALBEGIN ~ 1, data = df_wages)
stargazer(I2,  header=FALSE, type='latex')
resid_LOGSALBEGIN <- resid(I2)

I3 <- lm(LOGSAL ~ 1, data = df_wages)
stargazer(I3,  header=FALSE, type='latex')
resid_LOGSAL <- resid(I3)

I4 <- lm(EDUC ~ 1, data = df_wages)
stargazer(I4, header=FALSE, type='latex')
resid_EDUC <- resid(I4)
```
```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE , include = FALSE}
df_wages <- df_wages %>%
  mutate(DMLogSal = df_wages$LOGSAL-mean(df_wages$LOGSAL),
         DMeducation = df_wages$EDUC-mean(df_wages$EDUC),
         DMLogSalBegin = df_wages$LOGSALBEGIN-mean(df_wages$LOGSALBEGIN))
```

Results of the regression of the model $\overline{LogSal}$ $= \beta_1$ $\overline{education}$ $+ \beta_2$ $\overline{LogSalBegin}$ + $\epsilon$ are shown in *Table 6*.        

Results of the regresion of the model $DMLogSal = \beta_1 DMeducation + \beta_2 DMLogSalBegin + \epsilon$ are shown in *Table 7*.

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
reg_resid <- lm(resid_LOGSAL ~ 0 + resid_EDUC + resid_LOGSALBEGIN) 
stargazer(reg_resid,  header=FALSE, type='latex')

reg_DM <- lm(DMLogSal ~ 0 + DMeducation + DMLogSalBegin, data = df_wages) 
stargazer(reg_DM,  header=FALSE, type='latex')
```


\vspace{10pt}

### a) 

As we can see in *Table 6* and *Table 7*, the estimated coefficients are the same. Proove that residuals is the same as demean.

### b) 

Proove that demin without constant ad demean with constant leads to the same estimates.

\vspace{20pt}

## Question 4 

We test the model: $LogSal = \beta_1 + \beta_2education + \beta_3LogSalBegin + \beta_4gender + \beta_5minority + \epsilon$ (**R3**). Results are shown in *Table 8*.

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
R3 <- lm(LOGSAL ~ EDUC + LOGSALBEGIN + GENDER + MINORITY, data = df_wages)
stargazer(R3,  header=FALSE, type='latex')
```

```{r, include=FALSE }
anova(R3)
```

We can make a student test: $H_0: \beta_5 = 0$ versus $H_1: \beta_5 \neq 0$. The test statistic is:  $t_{\hat{\beta_5}}= {\hat{\beta_5} \over \hat{\sigma_5} }  \sim t_{0,975}(474-4-1)$.

We have: $t_{\hat{\beta_5}}= {\hat{\beta_5} \over \hat{\sigma_5} }= {-0,042 \over 0,020}\simeq -2.1$.

Hence, $\mid t_{\hat{\beta_5}} \mid = 2.1 > t(469) \in [1,960;1,984]$. 
We rejct $H_0$, which means that the variable *minority* is significant and relevant to explain wages.

\vspace{20pt}

## Question 5

We can make a Fisher test and test the hypothesis: $H_0: \beta_4=\beta_5=0$.
We can rewrite the hypothesis: $H_0$: $\quad \begin{pmatrix} 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ \end{pmatrix}$ $\quad \begin{pmatrix} \beta_1 \\ \beta_2\\ \beta_3\\\beta_4\\\beta_5\\ \end{pmatrix}$ =$\quad \begin{pmatrix} 0 \\ 0 \\ 0\\  0\\ 0\\ \end{pmatrix}$.       

```{r, include= FALSE}
Q5 <- lm(LOGSAL ~ EDUC + SALBEGIN, data = df_wages)
anova(Q5)
```

We can test whether this hypothesis is accepted or reject by making a Fisher test. The statistic is:     

$F= {RSS_{c} - RSS_{nc} \over RSS_{nc}}.{df_{c} \over df_c -df_{nc}} = {R^2_{nc}-R^2_{c} \over 1-R^2_{nc}}.{df_{c} \over df_c -df_{nc}}$


We have that SSR of the constrained model is equal to 18.532 (with 471 df). The SSR of the unconstrained model is 16,900 (with 469 df).     

Hence, $\hat{F} = {18,532 - 14.627 \over 14.627}.{469 \over 471 - 469}=59,97$ > $F(2,469)\in [3,01;3,03]$. We then reject $H_0$, which means that the variables *minority* and *gender* are jointly statistically significant.

\vspace{20pt}

## Question 6

We can make a Chow test to test whether the effect of one more year of education is the same for both groups (a group with at most 16 years of education and the other with at least 17 years of education).

```{r, include= FALSE}
df_below16 <- df_wages %>%
   filter(EDUC <= "16")
```

```{r, include= FALSE}
df_above17 <- df_wages %>%
   filter(EDUC >= "17")
```

```{r, include= FALSE}
Q61 <- lm(LOGSAL ~ EDUC + SALBEGIN + GENDER + MINORITY, data = df_below16)
anova(Q61)
```

```{r, include= FALSE}
Q62 <- lm(LOGSAL ~ EDUC + SALBEGIN + GENDER + MINORITY, data = df_above17)
anova(Q62)
```

Let us call:

- S the residual sum of squares of the global model **R3**;
- $S_{\le 16}$ the residual sum of squares of the same model but with observations of individuals whom education is below 16 years: $LogSal_{\le 16} = \gamma_1 + \gamma_2education_{\le 16} + \gamma_3LogSalBegin_{\le 16} + \gamma_4gender_{\le 16} + \gamma_5minority_{\le 16}+ \epsilon_{\le 16}$ ;
- $S_{\ge 17}$ the resiual sum of squares of the same model but with observations of individuals whom education is above 17 years: $LogSal_{\ge 17} = \eta_1 + \eta_2education_{\ge 17} + \eta_3LogSalBegin_{\ge 17} + \eta_4gender_{\ge 17} + \eta_5minority_{\ge 17}+ \epsilon_{\ge 17}$.

We test whether the hypothesis $H_0: \beta_i = \gamma_i = \eta_i, \forall i \in  [\![1;4]\!]$ is satisfied or not.       

The statistic of the Fisher test is:           

\begin{center}
$\hat{F}= {S - (S_{\le 16} + S_{\ge 17}) \over (S_{\le 16} + S_{\ge 17})}.{N-2(K+1) \over K}$  
\end{center}

We then have: $\hat{F}={16.900-(12.4866 + 3.3569) \over (12.4866 + 3.3569)}.{474-2(4+1) \over 4} = 7.735286 > F(5,464) \in [2.23;2,24]$. Hence, we reject $H_0$, which means that $\exists i \in [\![1;4]\!]$ such that $\beta_i \ne \gamma_i \ne \eta_i$.           

If we focus only on the regression of LogSal on education, we obtain that:  $\hat{F}={38.424-(23.523 + 6.4651) \over (23.523 + 6.4651)}.{474-2(1+1) \over 2} = 66.1074 > F(2,470) \in [3.01;3.03]$. Hence, we reject $H_0$, which means that the effect of *education* in the first group is not the same as the effect in the second groupe.         

\newpage

# 3. Heteroscedasticity

\vspace{20pt}

## Question 1

I think it is likely that the variation in wages differs among these categories. It might be the case that the variation in lower in custodial jobs and in administratives jobs than in management jobs, because:
- make plots
- compute variances

\vspace{20pt}

## Question 2

We estimate the model: $LogSal = \beta_1 + \beta_2 education + \beta_3 gender + \beta_4 minority + \beta_5 JobCat_2 + \beta_6 JobCat_3 + \varepsilon$ (**R4**)

We get (*Table 9*):

```{r, include=FALSE}
df_wages <- df_wages %>%
  mutate(JOBCAT = as.factor(JOBCAT),
         JOBCAT1 = ifelse(JOBCAT==1, 1, 0),
         JOBCAT2 = ifelse(JOBCAT==2, 1, 0), 
         JOBCAT3 = ifelse(JOBCAT==3, 1, 0))
```

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
R4 <- lm(LOGSAL ~ EDUC + GENDER + MINORITY + JOBCAT2 + JOBCAT3, data = df_wages)
stargazer(R4,  header=FALSE, type='latex')

resid_R4 <- residuals(R4)
df_wages <- cbind(df_wages, resid_R4)
```

**a)**    

We do not include a dummy for the first job category because If we do so, we would have that the first column of the X matrix, which is a vector composed of 1, is equal to the sum of the variables $JobCat_1$, $JobCat_2$ and $JobCat_3$. This leads to a problem for estimation because the matrix $X'X$ won't be invertible anymore because it is not a full rank matrix.      
 
\vspace{10pt}   

The parameters $\beta_5$ and $\beta_6$ are interepreted as the additionnal effect on the log of salary of having a job corresponding to, respectively, category 2 and 3.

\vspace{20pt}

**b)** 

```{r, include= FALSE}
df_jc1 <- df_wages %>%
  filter(JOBCAT=="1")
df_jc2 <- df_wages %>%
  filter(JOBCAT=="2")
df_jc3 <- df_wages %>%
  filter(JOBCAT=="3")
```

```{r , fig.width = 4, fig.height = 3, warning = FALSE, echo = FALSE, error = FALSE, message = FALSE, fig.align='center'}
plotCat1 <- ggplot(df_jc1, aes(x=LOGSAL, y=resid_R4))  +
  theme_bw() +
  labs(x = "Log Salary") +
  labs(y=" Residuals ") +
  labs(title = "Job Category 1") +
  geom_point(colour="red") 

plotCat1

plotCat2 <- ggplot(df_jc2, aes(x=LOGSAL, y=resid_R4))  +
  theme_bw() +
  labs(x = "Log Salary") +
  labs(y=" Residuals ") +
  labs(title = " Job Category 2") +
  geom_point(colour="red") 

plotCat2

plotCat3 <- ggplot(df_jc3, aes(x=LOGSAL, y=resid_R4))  +
  theme_bw() +
  labs(x = "Log Salary") +
  labs(y=" Residuals ") +
  labs(title = " Job Category 3") +
  geom_point(colour="red") 

plotCat3
```

We can conclude from these plots that there is an increasing function between the log of the salary and the residuals: the more the log salary, the more we over estimate it.

\vspace{30pt}

## Question 3

We estimate the model: $LogSal = \beta_1 + \beta_2 education + \beta_3 gender + \beta_4 minority   + \varepsilon$ (**R5**).

For subsample $n_1$ that includes employees of administratives jobs:

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
R51 <- lm(LOGSAL ~ EDUC + GENDER + MINORITY, data = df_jc1)
stargazer(R51,  header=FALSE, type='latex')
```

```{r, include=FALSE}
anova(R51)
```

For subsample $n_2$ that includes employees of management jobs:

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
R52 <- lm(LOGSAL ~ EDUC + GENDER + MINORITY, data = df_jc3)
stargazer(R52,  header=FALSE, type='latex')
```

```{r, include=FALSE}
anova(R52)
```

The Goldfeld-Quandt test is the most general test in which we assume there are 2 types of individuals (the ones who have a management job and the others who have an administrative job).

We find that: $\hat{\sigma_1}^2= \frac{12.7142}{363-3-1} = 0.0354156$ and $\hat{\sigma_2}^2= \frac{4.1396}{84-3-1} = 0.051745$.

We now perform a unilateral test:    

$H_0: \sigma_1^2 = \sigma_2^2$ \\
$H_1: \sigma_1^2 \ne \sigma_2^2$ \\

The test statistic is: $\hat{F}=\frac{\hat{\sigma_2}^2}{\hat{\sigma_1}^2} \underset{H_0}{\hookrightarrow} F(n_2-K,n1-k)$.

We have: $\hat{F}=\frac{0.051745}{0.0354156} = 1.461079 >F(80,359) \in [1.30;1.32]$. 

Hence, we reject $H_0$ which means that $\sigma_1^2 \ne \sigma_2^2$. We consider that there is heteroscedasticity in perturbations. 

\vspace{30pt}

## Question 4

The effects of the presence of heteroskedasticity on the asymptotic properties of the OLS estimator are :

- the estimator is still convergent and unbiased
- it is still asymptotically normal

\vspace{30pt}

## Question 5

```{r, echo = FALSE, results='asis', warning=FALSE, error=FALSE, message=FALSE }
WhiteSE_R4 <- vcovHC(R4, type = "HC0")
R4White <- coeftest(R4, WhiteSE_R4)
stargazer(R4White,  header=FALSE, type='latex')
```













